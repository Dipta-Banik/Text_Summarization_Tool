{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMg3+lIWVBjOtQIkyOljfKK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["%%writefile app.py\n","import nltk\n","import re\n","from nltk.corpus import stopwords, wordnet\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk import pos_tag\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import spacy\n","import heapq\n","import streamlit as st\n","\n","\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","\n","def get_wordnet_pos(tag):\n","    if tag.startswith('J'):\n","        return wordnet.ADJ\n","    elif tag.startswith('V'):\n","        return wordnet.VERB\n","    elif tag.startswith('N'):\n","        return wordnet.NOUN\n","    elif tag.startswith('R'):\n","        return wordnet.ADV\n","    else:\n","        return wordnet.NOUN\n","\n","def preprocess_text(sentence):\n","\n","    words = word_tokenize(re.sub('[^a-zA-Z]', ' ', sentence.lower()))\n","\n","    stop_words = set(stopwords.words(\"english\"))\n","    words = [word for word in words if word not in stop_words]\n","\n","    lemmatizer = WordNetLemmatizer()\n","    lemmatized_words = [\n","        lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n","        for word, tag in pos_tag(words)\n","    ]\n","    return \" \".join(lemmatized_words)\n","\n","def extract_important_entities(paragraph):\n","    doc = nlp(paragraph)\n","    entities = [ent.text for ent in doc.ents]\n","    return entities\n","\n","def summarize_text(paragraph):\n","    sentences = sent_tokenize(paragraph)\n","\n","    processed_sentences = [preprocess_text(sentence) for sentence in sentences]\n","\n","    entities = extract_important_entities(paragraph)\n","\n","    tfidf = TfidfVectorizer()\n","    tfidf_matrix = tfidf.fit_transform(processed_sentences)\n","    word_scores = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n","\n","    sentence_scores = {}\n","    for i, sentence in enumerate(sentences):\n","        words = word_tokenize(sentence.lower())\n","        score = sum(word_scores.get(word, 0) for word in words if word in word_scores)\n","        for entity in entities:\n","            if entity.lower() in sentence.lower():\n","                score += 1\n","        sentence_scores[sentence] = score\n","\n","\n","    top_sentences = heapq.nlargest(3, sentence_scores, key=sentence_scores.get)\n","    summary = ' '.join(top_sentences)\n","    return summary\n","\n","\n","\n","st.set_page_config(page_title=\"Text Summarization Tool\")\n","\n","if \"summary_history\" not in st.session_state:\n","    st.session_state.summary_history = []\n","\n","st.title('ü§ñ Text Summarization Tool')\n","st.subheader('Input your text below and get a summary üí°')\n","\n","input_text = st.text_area('Enter Text:', height=200)\n","\n","submit = st.button(\"Generate Summaryü™Ñ\")\n","clear_text = st.button(\"Clear Text üßπ\")\n","clear_history = st.button(\"Clear History üóëÔ∏è\")\n","\n","\n","if submit:\n","    if input_text.strip():\n","        summary = summarize_text(input_text)\n","        st.session_state.summary_history.append(summary)\n","        st.subheader('Summary:')\n","        st.write(summary)\n","    else:\n","        st.warning(\"Please enter some text to summarize.\")\n","\n","if clear_text:\n","    st.session_state.input_text = \"\"\n","    st.session_state.summary = \"\"\n","\n","if clear_history:\n","    st.session_state.summary_history = []\n","    st.success(\"Summary history cleared.\")\n","\n","st.sidebar.header(\"Summary History\")\n","if st.session_state.summary_history:\n","    for idx, summary in enumerate(st.session_state.summary_history):\n","        st.sidebar.write(f\"Summary {idx + 1}:\")\n","        st.sidebar.write(summary)\n","        st.sidebar.write(\"_\" * 50)\n","else:\n","    st.sidebar.write(\"No summaries generated yet.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VOv2g55dMWdO","executionInfo":{"status":"ok","timestamp":1735263851485,"user_tz":-330,"elapsed":401,"user":{"displayName":"Dipta Banik","userId":"04168264672836977736"}},"outputId":"c6a67c76-d7a2-47cb-9f1e-b90403af6756"},"execution_count":105,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]},{"cell_type":"code","source":["!wget -q -O - ipv4.icanhazip.com"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zl-mXv2hWQeu","executionInfo":{"status":"ok","timestamp":1735263855639,"user_tz":-330,"elapsed":365,"user":{"displayName":"Dipta Banik","userId":"04168264672836977736"}},"outputId":"7db62113-80f7-4c65-8a70-eac881f4bdd8"},"execution_count":106,"outputs":[{"output_type":"stream","name":"stdout","text":["34.106.229.208\n"]}]},{"cell_type":"code","source":["! streamlit run app.py & npx localtunnel --port 8501"],"metadata":{"id":"VG1huCCsX2tq"},"execution_count":null,"outputs":[]}]}